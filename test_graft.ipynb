{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rdaroya_umass_edu/miniconda3/envs/ann-ssc/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from os.path import join, isfile, isdir\n",
    "from os import listdir\n",
    "from multiprocessing import Pool\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from shutil import copy\n",
    "import torch.nn as nn\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "import urllib.request\n",
    "from transformers import CLIPVisionModelWithProjection, CLIPTextModelWithProjection, AutoTokenizer\n",
    "import torch\n",
    "# import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "import io\n",
    "\n",
    "from torch.utils.data.dataset import Dataset\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import fnmatch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pdb\n",
    "import random\n",
    "from loguru import logger\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_with_progress_bar(url, output_path):\n",
    "    with urllib.request.urlopen(url) as response:\n",
    "        total_size = int(response.headers.get('Content-Length', 0))\n",
    "        block_size = 1024  # 1 Kibibyte\n",
    "        progress_bar = tqdm(total=total_size, unit='iB', unit_scale=True)\n",
    "        with open(output_path, 'wb') as file:\n",
    "            while True:\n",
    "                data = response.read(block_size)\n",
    "                if not data:\n",
    "                    break\n",
    "                file.write(data)\n",
    "                progress_bar.update(len(data))\n",
    "\n",
    "        progress_bar.close()\n",
    "\n",
    "    print(f\"File downloaded and saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRAFT(nn.Module):\n",
    "    def __init__(self, CLIP_version=\"openai/clip-vit-base-patch16\", temp=False, bias_projector=True):\n",
    "        super().__init__()\n",
    "        # satellite image backbone\n",
    "        self.satellite_image_backbone = CLIPVisionModelWithProjection.from_pretrained(CLIP_version)\n",
    "        self.patch_size = self.satellite_image_backbone.config.patch_size\n",
    "\n",
    "        self.projector = nn.Sequential(\n",
    "            nn.LayerNorm(self.satellite_image_backbone.config.hidden_size, eps=self.satellite_image_backbone.config.layer_norm_eps),\n",
    "            nn.Linear(self.satellite_image_backbone.config.hidden_size, self.satellite_image_backbone.config.projection_dim, bias=bias_projector),\n",
    "        )\n",
    "        self.patch_size = self.satellite_image_backbone.config.patch_size\n",
    "        self.norm_dim = -1\n",
    "\n",
    "        self.temp = temp\n",
    "        if temp:\n",
    "            self.register_buffer(\"logit_scale\", torch.ones([]) * (1 / 0.07))\n",
    "\n",
    "    def forward(self, image_tensor):\n",
    "        # Extract features from satellite images\n",
    "        # B x 197 x 768 for VIT-B/16\n",
    "        hidden_state = self.satellite_image_backbone(image_tensor).last_hidden_state\n",
    "        # B x 197 x 512\n",
    "        satellite_image_features = F.normalize(self.projector(hidden_state), dim=self.norm_dim)\n",
    "        # get the satellite image features\n",
    "        return satellite_image_features\n",
    "\n",
    "    def forward_features(self, image_tensor):\n",
    "        # Extract features from satellite images\n",
    "        # B x 512 for VIT-B/16\n",
    "        embed = self.satellite_image_backbone(image_tensor).image_embeds\n",
    "        # B x 512\n",
    "        satellite_image_features = F.normalize(embed)\n",
    "        return satellite_image_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = 'https://graft.cs.cornell.edu/static/models/<PLACEHOLDER>/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1.03G/1.03G [00:09<00:00, 104MiB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File downloaded and saved to checkpoints/graft/sentinel_model.ckpt\n"
     ]
    }
   ],
   "source": [
    "device=\"cuda\"\n",
    "model = GRAFT(temp=True, bias_projector=False).to(device)\n",
    "ckpt_url = model_dir+'/graft_sentinel.ckpt'\n",
    "output_path = \"checkpoints/graft/sentinel_model.ckpt\"\n",
    "ckpt_file = download_with_progress_bar(ckpt_url, output_path)\n",
    "transform = transforms.Compose([transforms.Resize((224, 224)),\n",
    "  # transforms.ToTensor(),\n",
    "  transforms.Normalize(mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])])\n",
    "\n",
    "ckpt_path = \"checkpoints/graft/sentinel_model.ckpt\"\n",
    "sd = torch.load(ckpt_path)\n",
    "model.load_state_dict(sd['state_dict'], strict=False)\n",
    "textmodel = CLIPTextModelWithProjection.from_pretrained(\"openai/clip-vit-base-patch16\").eval().to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"openai/clip-vit-base-patch16\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationDataset(Dataset):\n",
    "    \"\"\"\n",
    "    This file is directly modified from https://pytorch.org/docs/stable/torchvision/datasets.html\n",
    "    \"\"\"\n",
    "    def __init__(self, dataset_name:str=\"eurosat\", split='test'):\n",
    "        self.split = split\n",
    "\n",
    "        # Read the data file\n",
    "        df = pd.read_csv(f\"../data/{dataset_name}_data.csv\")\n",
    "        df = df[df[\"split\"]==\"test\"]\n",
    "\n",
    "        with open(f\"../data/{dataset_name}_metadata.npy\", \"rb\") as f:\n",
    "            self.label_to_class = np.load(f, allow_pickle=True)[()][\"classes\"]  # maps from idx (0-9) to actual string\n",
    "        self.fps = df[\"fp\"].values\n",
    "        self.labels = df[\"label\"].values\n",
    "        print(f\"label and class mapping: \\n{df[['class_name','label']].value_counts().sort_index()}\")\n",
    "        self.num_outputs = len(self.label_to_class)\n",
    "      \n",
    "        self.data_len = len(self.fps)\n",
    "\n",
    "        self.transforms_list = [transforms.ToTensor()]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        fp = self.fps[index]\n",
    "        label = self.labels[index]\n",
    "        image = Image.open(fp)\n",
    "\n",
    "        data_transforms = transforms.Compose(self.transforms_list)\n",
    "        image = data_transforms(image)  # output of transforms: (3, s, s)\n",
    "\n",
    "        return (\n",
    "            image,    # shape: (3, s, s)\n",
    "            label,\n",
    "        )\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data_len\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label and class mapping: \n",
      "class_name            label\n",
      "AnnualCrop            0        566\n",
      "Forest                1        602\n",
      "HerbaceousVegetation  2        593\n",
      "Highway               3        515\n",
      "Industrial            4        485\n",
      "Pasture               5        390\n",
      "PermanentCrop         6        486\n",
      "Residential           7        628\n",
      "River                 8        504\n",
      "SeaLake               9        596\n",
      "Name: count, dtype: int64\n",
      "transform from dataset: [ToTensor()]\n",
      "['AnnualCrop', 'Forest', 'HerbaceousVegetation', 'Highway', 'Industrial', 'Pasture', 'PermanentCrop', 'Residential', 'River', 'SeaLake']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test_dataset1 = ClassificationDataset(dataset_name=\"eurosat\", split=\"test\")\n",
    "print(\"transform from dataset:\", test_dataset1.transforms_list)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "        test_dataset1,\n",
    "        batch_size=16,\n",
    "        shuffle=False,\n",
    "        num_workers=1,\n",
    "        pin_memory=True,\n",
    "        sampler=None,\n",
    "        drop_last=False,\n",
    ")\n",
    "test_dataset = iter(test_loader)\n",
    "label_to_class = test_dataset1.label_to_class\n",
    "print(label_to_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A photo of a annual crop', 'A photo of a forest', 'A photo of a herbaceous vegetation', 'A photo of a highway', 'A photo of a industrial', 'A photo of a pasture', 'A photo of a permanent crop', 'A photo of a residential', 'A photo of a river', 'A photo of a sea lake']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 512])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map_clean_text ={\n",
    "    'AnnualCrop': \"annual crop\",\n",
    "    'Forest': \"forest\", \n",
    "    'HerbaceousVegetation': \"herbaceous vegetation\", \n",
    "    'Highway': 'highway', \n",
    "    'Industrial': 'industrial', \n",
    "    'Pasture': 'pasture', \n",
    "    'PermanentCrop': 'permanent crop', \n",
    "    'Residential': 'residential', \n",
    "    'River': 'river', \n",
    "    'SeaLake': 'sea lake',\n",
    "}\n",
    "label_to_class = [f\"A photo of a {map_clean_text[k]}\" for k in label_to_class]\n",
    "# label_to_class = [f\"A centered satellite photo of a {map_clean_text[k]}\" for k in label_to_class]\n",
    "print(label_to_class)\n",
    "textsenc = tokenizer(label_to_class, padding=True, return_tensors=\"pt\").to(device)\n",
    "text_features = F.normalize(textmodel(**textsenc).text_embeds, dim=-1)\n",
    "text_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 336/336 [00:45<00:00,  7.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.27642124883504193\n",
      "num_total: 5365\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_batch = len(test_loader)\n",
    "num_correct, num_total = 0,0\n",
    "for k in tqdm(range(test_batch)):\n",
    "  imgs, labels = next(test_dataset)\n",
    "  with torch.no_grad():\n",
    "    tr_image = transform(imgs).to(device)\n",
    "    image_feature = model.forward_features(tr_image)\n",
    "    texts = label_to_class\n",
    "  with torch.no_grad():\n",
    "    textsenc = tokenizer(texts, padding=True, return_tensors=\"pt\").to(device) #tokenize\n",
    "    class_embeddings = F.normalize(textmodel(**textsenc).text_embeds, dim=-1) #embed with text encoder\n",
    "  classlogits = image_feature.cpu().numpy() @ class_embeddings.cpu().numpy().T\n",
    "  \n",
    "  preds = np.argmax(classlogits, axis=1)\n",
    "  labels = labels.detach().numpy()\n",
    "  num_cor = np.sum(preds==labels)\n",
    "  num_correct += num_cor\n",
    "  num_total += len(labels)\n",
    "print(f\"Accuracy: {num_correct/num_total}\")\n",
    "print(f\"num_total: {num_total}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_shot_classification(image, classes=None):\n",
    "  with torch.no_grad():\n",
    "    tr_image = transform(image).unsqueeze(0).to(device)\n",
    "    image_feature = model.forward_features(tr_image)\n",
    "  if classes is not None:\n",
    "    texts = classes\n",
    "  else:\n",
    "    texts = [\"tennis courts\", \"parking lot\", \"farmland\", \"lake\", \"park\", \"powerlines\", \"University Campus\", \"Beach\", \"Freeway\"]\n",
    "  with torch.no_grad():\n",
    "    textsenc = tokenizer(texts, padding=True, return_tensors=\"pt\").to(device) #tokenize\n",
    "    class_embeddings = F.normalize(textmodel(**textsenc).text_embeds, dim=-1) #embed with text encoder\n",
    "  classlogits = image_feature.cpu().numpy() @ class_embeddings.cpu().numpy().T\n",
    "  fig = plt.figure(figsize=(10, 5))\n",
    "  plt.subplot(1, 2, 1)\n",
    "  plt.title(\"Input Image\")\n",
    "  plt.imshow(image)\n",
    "  plt.axis('Off')\n",
    "  plt.subplot(1, 2, 2)\n",
    "  plt.ylabel('Class matching score')\n",
    "  plt.xlabel('Classes')\n",
    "  plt.title(\"Graft best prediction: '{}'\".format(texts[np.argmax(classlogits[0])]))\n",
    "  plt.bar(range(len(classlogits[0])), classlogits[0])\n",
    "  plt.xticks(range(len(texts)), texts, rotation=90)\n",
    "  plt.show()\n",
    "  return classlogits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ann-ssc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
